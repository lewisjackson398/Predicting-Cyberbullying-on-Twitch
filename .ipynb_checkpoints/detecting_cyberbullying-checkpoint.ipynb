{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Chat              Label\n",
      "0                                  who the fucks emong      cyberbullying\n",
      "1                             BAN THIS FUCKING R-TARD!      cyberbullying\n",
      "2                    USE YOUR FUCKING FLASHBANG PLEASE      cyberbullying\n",
      "3                     DOC JUST FUCKIN DO IT OKAY 4Head      cyberbullying\n",
      "4                  will you shut the fuck up for a bit      cyberbullying\n",
      "..                                                 ...                ...\n",
      "995                        You are gorgeous I love you  non-cyberbullying\n",
      "996            And you can show your dog your <3 <3 <3  non-cyberbullying\n",
      "997                                   u look fantastic  non-cyberbullying\n",
      "998  you look like scarlet Johnson i bet you here t...  non-cyberbullying\n",
      "999      you're looking so great, love your streams <3  non-cyberbullying\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"test_data.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Vocabulary:  1068\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    #only alphabet allowed\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['Chat'][i])\n",
    "    #set to lowercase\n",
    "    review = review.lower()\n",
    "    #tokenize\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "#bag of words transformer\n",
    "bow_transformer = CountVectorizer(stop_words='english')\n",
    "bow_transformer = bow_transformer.fit(corpus)\n",
    "\n",
    "print('Length of the Vocabulary: ',len(bow_transformer.vocabulary_))\n",
    "messages_bow = bow_transformer.transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "X = tfidf_transformer.transform(messages_bow)\n",
    "\n",
    "y = []\n",
    "for row in df[\"Label\"]:\n",
    "    y.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print idf values\n",
    "#df_idf = pd.DataFrame(tfidf_transformer.idf_, index=bow_transformer.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "#print(df_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection - setting train and test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "\n",
    "x_train_std = sc.fit_transform(X_train)\n",
    "x_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results\n",
      "Accuracy: 0.97 \n",
      "\n",
      "Confusion Matrix\n",
      " [[158   7]\n",
      " [  3 162]]\n",
      "\n",
      "F1 Score:  0.9696925168068771\n",
      "Precision Score:  0.9699731706420669\n",
      "Recall Score:  0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Naive Bayes Results')\n",
    "print('Accuracy: %.2f ' %accuracy_score(y_test,y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "print('\\nF1 Score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Results\n",
      "Accuracy: 0.98 \n",
      "\n",
      "Confusion Matrix\n",
      " [[157   8]\n",
      " [  0 165]]\n",
      "\n",
      "F1 Score:  0.9757433202249255\n",
      "Precision Score:  0.976878612716763\n",
      "Recall Score:  0.9757575757575758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Decision Tree Results')\n",
    "print('Accuracy: %.2f ' %accuracy_score(y_test,y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "print('\\nF1 Score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results\n",
      "Accuracy: 0.99 \n",
      "\n",
      "Confusion Matrix\n",
      " [[163   2]\n",
      " [  0 165]]\n",
      "\n",
      "F1 Score:  0.9939391713194241\n",
      "Precision Score:  0.9940119760479043\n",
      "Recall Score:  0.9939393939393939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Random Forest Results')\n",
    "print('Accuracy: %.2f ' %accuracy_score(y_test,y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "print('\\nF1 Score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "Accuracy: 0.99 \n",
      "\n",
      "Confusion Matrix\n",
      " [[161   4]\n",
      " [  0 165]]\n",
      "\n",
      "F1 Score:  0.9878770067227509\n",
      "Precision Score:  0.9881656804733727\n",
      "Recall Score:  0.9878787878787878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1000.0, random_state=0, solver='lbfgs')\n",
    "lr.fit(x_train_std, y_train)\n",
    "y_pred = lr.predict(x_test_std)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Logistic Regression Results')\n",
    "print('Accuracy: %.2f ' %accuracy_score(y_test,y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "print('\\nF1 Score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Results\n",
      "Accuracy: 0.98 \n",
      "\n",
      "Confusion Matrix\n",
      " [[165   0]\n",
      " [  5 160]]\n",
      "\n",
      "F1 Score:  0.9848450057405281\n",
      "Precision Score:  0.9852941176470589\n",
      "Recall Score:  0.9848484848484849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10)\n",
    "svm.fit(x_train_std, y_train)\n",
    "y_pred = svm.predict(x_test_std)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('Support Vector Machine Results')\n",
    "print('Accuracy: %.2f ' %accuracy_score(y_test,y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "print('\\nF1 Score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Results\n",
      "Accuracy: 0.74 \n",
      "\n",
      "Confusion Matrix\n",
      " [[ 82  83]\n",
      " [  3 162]]\n",
      "\n",
      "F1 Score:  0.7231219512195122\n",
      "Precision Score:  0.8129651860744298\n",
      "Recall Score:  0.7393939393939394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors =5, metric = 'minkowski')\n",
    "knn.fit(x_train_std, y_train)\n",
    "y_pred = knn.predict(x_test_std)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "print('K-Nearest Neighbors Results')\n",
    "print('Accuracy: %.2f ' %accuracy_score(y_test,y_pred))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "print('\\nF1 Score: ', f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print('Precision Score: ', precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \",recall_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cyberbullying', 'non-cyberbullying'], dtype='<U17')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = ['You idiot', \"I like you alot\"]\n",
    "new_test = bow_transformer.transform(test_set)\n",
    "\n",
    "classifier.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
