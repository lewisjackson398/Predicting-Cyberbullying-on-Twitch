{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"cleanprojectdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Tweet    Text Label\n",
      "0     .omg why are poc wearing fugly blue contacts s...  Non-Bullying\n",
      "1     .Sorry but most of the runners popular right n...  Non-Bullying\n",
      "2     .those jeans are hideous, and I?m afraid he?s ...  Non-Bullying\n",
      "3     .I had to dress up for a presentation in class...  Non-Bullying\n",
      "4     .Am I the only one who thinks justin bieber is...  Non-Bullying\n",
      "...                                                 ...           ...\n",
      "1060  No we are not, But you are a race baiting libt...      Bullying\n",
      "1061  you wont get anyone for this challenge., after...      Bullying\n",
      "1062  I will follow you if you are not a libtard,Mus...      Bullying\n",
      "1063  michaelianblack Ur a child, an ostrich w/ your...      Bullying\n",
      "1064  FoxNews. not to all the ppl I know that live t...      Bullying\n",
      "\n",
      "[1065 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words and labels into lists\n",
    "Tweet = []\n",
    "Labels = []\n",
    "\n",
    "for row in df1[\"Tweet\"]:\n",
    "    #tokenize words\n",
    "    words = word_tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    Tweet.append(lemma_list)\n",
    "\n",
    "    for row in df1[\"Text Label\"]:\n",
    "        Labels.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine them to create bag of words\n",
    "combined = zip(Tweet, Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create bag of words and dictionary object\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key, Value Pair into new list for modeling\n",
    "Final_Data = []\n",
    "for r, v in combined:\n",
    "    bag_of_words(r)\n",
    "    Final_Data.append((bag_of_words(r),v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n"
     ]
    }
   ],
   "source": [
    "#random shuffle\n",
    "import random\n",
    "random.shuffle(Final_Data)\n",
    "print(len(Final_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training set and testing 60/40 split\n",
    "train_set, test_set = Final_Data[0:746], Final_Data[746:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Unigrams \n",
      "Accuracy: 0.6394984326018809\n"
     ]
    }
   ],
   "source": [
    "#import confusion matrix metrics and run Naive Bayes with Unigrams\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "#find accuracy\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "print(\"Naive Bayes Performance with Unigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramNB Recall\n",
      "Bullying recall: 0.5542168674698795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find recall\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "nbrefset = collections.defaultdict(set)\n",
    "nbtestset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    nbrefset[label].add(i)\n",
    "    observed = nb_classifier.classify(feats)\n",
    "    nbtestset[observed].add(i)\n",
    "    \n",
    "print(\"UnigramNB Recall\")\n",
    "print('Bullying recall:', recall(nbtestset['Bullying'], nbrefset['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    dumb = True           Bullyi : Non-Bu =      8.7 : 1.0\n",
      "                   pussy = True           Bullyi : Non-Bu =      6.0 : 1.0\n",
      "                  retard = True           Bullyi : Non-Bu =      6.0 : 1.0\n",
      "               worthless = True           Bullyi : Non-Bu =      5.9 : 1.0\n",
      "                 someone = True           Non-Bu : Bullyi =      5.9 : 1.0\n",
      "                     big = True           Bullyi : Non-Bu =      5.6 : 1.0\n",
      "                    last = True           Bullyi : Non-Bu =      5.6 : 1.0\n",
      "               prejudice = True           Non-Bu : Bullyi =      5.6 : 1.0\n",
      "                     low = True           Bullyi : Non-Bu =      5.4 : 1.0\n",
      "                   loser = True           Bullyi : Non-Bu =      5.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#find most informative features\n",
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramDT Recall\n",
      "Bullying recall: 0.7083333333333334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run Decision Tree for Unigrams to find recall\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"UnigramDT Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramsLogit Recall\n",
      "Bullying recall: 0.6989247311827957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run Maxent Classifier for Unigrams\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"UnigramsLogit Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniigramSVM Recall\n",
      "Bullying recall: 0.6989247311827957\n"
     ]
    }
   ],
   "source": [
    "#Run Support Vector Machine for Unigrams\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(gamma='auto'), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"UniigramSVM Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same thing with bigrams\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words for bigrams\n",
    "def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)  \n",
    "    bigrams = bigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data2 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_bigrams_words(z)\n",
    "    Final_Data2.append((bag_of_bigrams_words(z),e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(Final_Data2)\n",
    "print(len(Final_Data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data again around 60/40\n",
    "train_set, test_set = Final_Data2[0:747], Final_Data2[747:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Bigrams \n",
      "Accuracy: 0.6163522012578616\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes for Bigrams\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "from nltk import metrics\n",
    "\n",
    "\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "#Accuracy\n",
    "\n",
    "print(\"Naive Bayes Performance with Bigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "       ('piece', 'shit') = True           Bullyi : Non-Bu =     15.3 : 1.0\n",
      "           ('low', 'iq') = True           Bullyi : Non-Bu =     14.7 : 1.0\n",
      "  ('worthless', 'piece') = True           Bullyi : Non-Bu =     10.0 : 1.0\n",
      "        ('fuck', 'goat') = True           Bullyi : Non-Bu =      2.6 : 1.0\n",
      "      ('nobody', 'care') = True           Bullyi : Non-Bu =      2.4 : 1.0\n",
      "     ('fucking', 'cunt') = True           Bullyi : Non-Bu =      2.2 : 1.0\n",
      "           ('i\\\\', \"'m\") = True           Non-Bu : Bullyi =      1.9 : 1.0\n",
      "           ('iq', 'low') = True           Bullyi : Non-Bu =      1.6 : 1.0\n",
      "('1', 'low-iq-arabists') = True           Bullyi : Non-Bu =      1.6 : 1.0\n",
      "        ('idiot', 'fag') = True           Bullyi : Non-Bu =      1.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Informative Features for Bigrams\n",
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramDT Recall\n",
      "Bullying recall: 0.7241379310344828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree for Bigrams\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"BigramDT Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramsLogit Recall\n",
      "Bullying recall: 0.6491228070175439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Maxent Classifier for Bigrams\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"BigramsLogit Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams Recall\n",
      "Bullying recall: 0.6491228070175439\n"
     ]
    }
   ],
   "source": [
    "#Support Vecotr Machine for Bigrams\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(gamma='auto'), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"Bigrams Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same thing with Trigrams\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "#Bag of words for Trigrams\n",
    "def bag_of_trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq, n=200):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)  \n",
    "    trigrams = trigram_finder.nbest(score_fn, n)  \n",
    "    return bag_of_words(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n"
     ]
    }
   ],
   "source": [
    "#Final list for modeling\n",
    "Final_Data3 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_trigrams_words(z)\n",
    "    Final_Data3.append((bag_of_trigrams_words(z),e))\n",
    "\n",
    "import random\n",
    "random.shuffle(Final_Data3)\n",
    "print(len(Final_Data3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60/40\n",
    "train_set, test_set = Final_Data3[0:747], Final_Data3[747:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes for Trigrams\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance with Trigrams \n",
      "Accuracy: 0.6037735849056604\n"
     ]
    }
   ],
   "source": [
    "#Accuracy\n",
    "print(\"Naive Bayes Performance with Trigrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bullying precision: 0.7142857142857143\n",
      "bullying recall: 0.03875968992248062\n"
     ]
    }
   ],
   "source": [
    "#Metrics\n",
    "print('bullying precision:', precision(refsets['Bullying'], testsets['Bullying']))\n",
    "print('bullying recall:', recall(refsets['Bullying'], testsets['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "('worthless', 'piece', 'shit') = True           Bullyi : Non-Bu =      9.5 : 1.0\n",
      "('low-iq-arabists', 'trying', 'smear') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "(\"don\\\\'t\", 'identify', 'w/arabism') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('smear', 'racist', 'minority') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('racist', 'minority', \"don\\\\'t\") = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('w/arabism', 'clueless', '+not') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('also', 'beating', 'woman') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('another', 'low', 'iq') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('identify', 'w/arabism', 'clueless') = True           Bullyi : Non-Bu =      1.5 : 1.0\n",
      "('beating', 'woman', 'idiot') = True           Bullyi : Non-Bu =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Most informative features for Trigrams\n",
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramDT Recall\n",
      "Bullying recall: 0.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree for Trigrams\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"TrigramDT Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrigramsLogit Recall\n",
      "Bullying recall: 0.7142857142857143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Maxent Classifier for Trigrams\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"TrigramsLogit Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams Recall\n",
      "Bullying recall: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine for Trigrams\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(gamma='auto'), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"Trigrams Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = zip(Tweet,Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Unigrams, Bigrams, and Trigrams for (N=3) modeling\n",
    "\n",
    "# Import Bigram metrics - we will use these to identify the top 200 trigrams\n",
    "def bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq,\n",
    "n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return bigrams\n",
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "\n",
    "# Import Trigram metrics - we will use these to identify the top 200 trigrams\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def trigrams_words(words, score_fn=TrigramAssocMeasures.chi_sq,\n",
    "n=200):\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "    trigrams = trigram_finder.nbest(score_fn, n)\n",
    "    return trigrams\n",
    "\n",
    "#Combined\n",
    "def bag_of_Ngrams_words(words):\n",
    "    bigramBag = bigrams_words(words)\n",
    "    \n",
    "    #The following two for loops convert tuple into string\n",
    "    for b in range(0,len(bigramBag)):\n",
    "        bigramBag[b]=' '.join(bigramBag[b])\n",
    "   \n",
    "    trigramBag = trigrams_words(words)\n",
    "    for t in range(0,len(trigramBag)):\n",
    "        trigramBag[t]=' '.join(trigramBag[t])\n",
    "        \n",
    " #New bag of words\n",
    "\n",
    "    return bag_of_words(trigramBag + bigramBag + words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Data4 =[]\n",
    "\n",
    "for z, e in combined:\n",
    "    bag_of_Ngrams_words(z)\n",
    "    Final_Data4.append((bag_of_Ngrams_words(z),e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065\n",
      "Naive Bayes Performance with Ngrams \n",
      "Accuracy: 0.6729559748427673\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes for Ngrams\n",
    "import random\n",
    "random.shuffle(Final_Data4)\n",
    "print(len(Final_Data4))\n",
    "\n",
    "train_set, test_set = Final_Data4[0:747], Final_Data4[747:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure) \n",
    "\n",
    "\n",
    "refsets = collections. defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    testsets[observed].add(i)\n",
    "\n",
    "#Accuracy\n",
    "print(\"Naive Bayes Performance with Ngrams \")    \n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              piece shit = True           Bullyi : Non-Bu =      9.9 : 1.0\n",
      "                  low iq = True           Bullyi : Non-Bu =      8.0 : 1.0\n",
      "                 libtard = True           Bullyi : Non-Bu =      7.5 : 1.0\n",
      "                   piece = True           Bullyi : Non-Bu =      7.5 : 1.0\n",
      "         worthless piece = True           Bullyi : Non-Bu =      6.9 : 1.0\n",
      "                  stupid = True           Bullyi : Non-Bu =      6.5 : 1.0\n",
      "                   sorry = True           Bullyi : Non-Bu =      6.5 : 1.0\n",
      "    worthless piece shit = True           Bullyi : Non-Bu =      6.3 : 1.0\n",
      "                     low = True           Bullyi : Non-Bu =      6.3 : 1.0\n",
      "               worthless = True           Bullyi : Non-Bu =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Informative features for Ngrams\n",
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bullying precision: 0.5740740740740741\n",
      "bullying recall: 0.7265625\n"
     ]
    }
   ],
   "source": [
    "print('bullying precision:', precision(refsets['Bullying'], testsets['Bullying']))\n",
    "print('bullying recall:', recall(refsets['Bullying'], testsets['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramDT Recall\n",
      "Bullying recall: 0.7454545454545455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree for Ngrams\n",
    "from nltk.classify import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier.train(train_set, \n",
    "                                             binary=True, \n",
    "                                             entropy_cutoff=0.8, \n",
    "                                             depth_cutoff=5, \n",
    "                                             support_cutoff=30)\n",
    "refset = collections.defaultdict(set)\n",
    "testset = collections.defaultdict(set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = dt_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"NgramDT Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NgramsLogit Recall\n",
      "Bullying recall: 0.5966386554621849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Maxent Classifier, Logistic Regression for Ngrams\n",
    "from nltk.classify import MaxentClassifier\n",
    "\n",
    "logit_classifier = MaxentClassifier.train(train_set, algorithm='gis', trace=0, max_iter=10, min_lldelta=0.5)\n",
    "\n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = logit_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "print(\"NgramsLogit Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrams Recall\n",
      "Bullying recall: 0.5966386554621849\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine for Ngrams\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SklearnClassifier(SVC(gamma='auto'), sparse=False).train(train_set)\n",
    " \n",
    "for i, (feats, label) in enumerate(test_set):\n",
    "    refset[label].add(i)\n",
    "    observed = SVM_classifier.classify(feats)\n",
    "    testset[observed].add(i)\n",
    "    \n",
    "print(\"Ngrams Recall\")\n",
    "print('Bullying recall:', recall(testset['Bullying'], refset['Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   piece = True           Bullyi : Non-Bu =     10.2 : 1.0\n",
      "               worthless = True           Bullyi : Non-Bu =      8.9 : 1.0\n",
      "                 libtard = True           Bullyi : Non-Bu =      7.7 : 1.0\n",
      "                     low = True           Bullyi : Non-Bu =      6.4 : 1.0\n",
      "                   lying = True           Bullyi : Non-Bu =      5.6 : 1.0\n",
      "                   whole = True           Bullyi : Non-Bu =      5.6 : 1.0\n",
      "                      ur = True           Bullyi : Non-Bu =      5.6 : 1.0\n",
      "                  retard = True           Bullyi : Non-Bu =      5.5 : 1.0\n",
      "                      iq = True           Bullyi : Non-Bu =      5.0 : 1.0\n",
      "                    stop = True           Non-Bu : Bullyi =      5.0 : 1.0\n",
      "0.6572327044025157\n",
      "bullying precision: 0.581081081081081\n",
      "bullying recall: 0.6466165413533834\n",
      "bullying F-measure: 0.6120996441281138\n",
      "not-bullying precision: 0.7235294117647059\n",
      "not-bullying recall: 0.6648648648648648\n",
      "not-bullying F-measure: 0.6929577464788732\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = Final_Data[0:747], Final_Data[747:]\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n",
    "nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nb_classifier.show_most_informative_features(10)\n",
    "\n",
    "from nltk.classify.util import accuracy\n",
    "print(accuracy(nb_classifier, test_set))\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "    \n",
    "for i, (Final_Data, label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    observed = nb_classifier.classify(Final_Data)\n",
    "    testsets[observed].add(i)\n",
    "    \n",
    "print('bullying precision:', precision(refsets['Bullying'], testsets['Bullying']))\n",
    "print('bullying recall:', recall(refsets['Bullying'], testsets['Bullying']))\n",
    "print('bullying F-measure:', f_measure(refsets['Bullying'], testsets['Bullying']))\n",
    "print('not-bullying precision:', precision(refsets['Non-Bullying'], testsets['Non-Bullying']))\n",
    "print('not-bullying recall:', recall(refsets['Non-Bullying'], testsets['Non-Bullying']))\n",
    "print('not-bullying F-measure:', f_measure(refsets['Non-Bullying'], testsets['Non-Bullying']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
